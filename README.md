<p align="center">
  <img src="https://github.com/"  width="100" height="100">
</p>

<br>
<br>

## Features
- Transformer
  - Multi-Head self attention
  - Adam (adaptive moment estimation) optimization
  - Multi-threaded gradient accumulator
- Activation
  - GELU (Gaussian error linear unit)
  - RELU (Rectified linear unit)
  - SiLU (Sigmoid linear unit)
  - Mish (Self regularized non-monotonic)
  - SwiGLU (Swish and GLU)
- Sampler
  - Top P&K with temperature
